{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 라이브러리 선언 및 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REGIONSEG1</th>\n",
       "      <th>PRODUCTSEG1</th>\n",
       "      <th>PRODUCTSEG2</th>\n",
       "      <th>REGIONSEG2</th>\n",
       "      <th>REGIONSEG3</th>\n",
       "      <th>PRODUCTSEG3</th>\n",
       "      <th>YEARWEEK</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>QTY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PG05</td>\n",
       "      <td>SALESID0001</td>\n",
       "      <td>SITEID0004</td>\n",
       "      <td>ITEM0445</td>\n",
       "      <td>201306</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PG05</td>\n",
       "      <td>SALESID0001</td>\n",
       "      <td>SITEID0004</td>\n",
       "      <td>ITEM0445</td>\n",
       "      <td>201307</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PG05</td>\n",
       "      <td>SALESID0001</td>\n",
       "      <td>SITEID0004</td>\n",
       "      <td>ITEM0445</td>\n",
       "      <td>201308</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PG05</td>\n",
       "      <td>SALESID0001</td>\n",
       "      <td>SITEID0004</td>\n",
       "      <td>ITEM0445</td>\n",
       "      <td>201309</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PG05</td>\n",
       "      <td>SALESID0001</td>\n",
       "      <td>SITEID0004</td>\n",
       "      <td>ITEM0445</td>\n",
       "      <td>201310</td>\n",
       "      <td>2013</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  REGIONSEG1  PRODUCTSEG1 PRODUCTSEG2   REGIONSEG2  REGIONSEG3 PRODUCTSEG3  \\\n",
       "0        A01          NaN        PG05  SALESID0001  SITEID0004    ITEM0445   \n",
       "1        A01          NaN        PG05  SALESID0001  SITEID0004    ITEM0445   \n",
       "2        A01          NaN        PG05  SALESID0001  SITEID0004    ITEM0445   \n",
       "3        A01          NaN        PG05  SALESID0001  SITEID0004    ITEM0445   \n",
       "4        A01          NaN        PG05  SALESID0001  SITEID0004    ITEM0445   \n",
       "\n",
       "   YEARWEEK  YEAR  WEEK  QTY  \n",
       "0    201306  2013     6    5  \n",
       "1    201307  2013     7    8  \n",
       "2    201308  2013     8    7  \n",
       "3    201309  2013     9    1  \n",
       "4    201310  2013    10    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## 1. 데이터 불러오기\n",
    "\n",
    "salesData = pd.read_csv(\"./dataset/pro_actual_sales.csv\")\n",
    "salesData.columns = salesData.columns.str.upper() \n",
    "salesData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 정제 (2년 이상 실적 자료 정제 및 53주차 제거된 풀데이터 셋)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x000001AD3A4D3548>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ##### 제품 별로 데이터 나누기\n",
    "\n",
    "groupkey = [\"REGIONSEG1\",\"REGIONSEG2\",\"REGIONSEG3\",\"PRODUCTSEG2\",\"PRODUCTSEG3\"]\n",
    "groupData = salesData.groupby(groupkey)['YEARWEEK','YEAR','WEEK','QTY']\n",
    "groupData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupsData = salesData.groupby(groupkey)[\"YEARWEEK\"].agg([\"count\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupsData = groupsData.rename(columns={\"count\":\"KNOB\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 데이터를 key(groupKey)와 data로 나누고 매칭시켜 딕셔너리로 변환할건데,  \n",
    "# seq_length를 고려해서 52주차 이상의 실적이 있는 데이터만 선별하여 담는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinData = pd.merge(left=salesData, right=groupsData,on=groupkey)\n",
    "\n",
    "maxKnob = 104\n",
    "\n",
    "cleansedData = joinData[joinData.KNOB >= maxKnob]\n",
    "\n",
    "len(cleansedData.PRODUCTSEG3.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalizing\n",
    "\n",
    "# newqty=[]\n",
    "\n",
    "# for i in range(0,len(each_data)):\n",
    "#     maxval = each_data[i][\"QTY\"].max()\n",
    "#     each_data[i][\"QTY\"].values.astype('float')\n",
    "#     tmpqty= each_data[i][\"QTY\"].values #/maxval\n",
    "#     newqty.append(tmpqty)\n",
    "    \n",
    "#     each_data[i][\"NEW_QTY\"]=\"\"\n",
    "#     each_data[i][\"NEW_QTY\"]=tmpqty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eachItem = cleansedData[(cleansedData.PRODUCTSEG3 ==\"ITEM0110\") &\n",
    "                        (cleansedData.REGIONSEG3 == \"SITEID0004\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eachItem2 = eachItem.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eachItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRODUCTSEG1</th>\n",
       "      <th>YEARWEEK</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>QTY</th>\n",
       "      <th>KNOB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201481.818182</td>\n",
       "      <td>2014.551948</td>\n",
       "      <td>26.623377</td>\n",
       "      <td>148.123377</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.519593</td>\n",
       "      <td>0.943102</td>\n",
       "      <td>15.300510</td>\n",
       "      <td>179.190202</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201331.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201417.250000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>13.250000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201503.500000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201541.750000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>139.250000</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201627.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1147.000000</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PRODUCTSEG1       YEARWEEK         YEAR        WEEK          QTY   KNOB\n",
       "count          0.0     154.000000   154.000000  154.000000   154.000000  154.0\n",
       "mean           NaN  201481.818182  2014.551948   26.623377   148.123377  154.0\n",
       "std            NaN      88.519593     0.943102   15.300510   179.190202    0.0\n",
       "min            NaN  201331.000000  2013.000000    1.000000     8.000000  154.0\n",
       "25%            NaN  201417.250000  2014.000000   13.250000    56.000000  154.0\n",
       "50%            NaN  201503.500000  2015.000000   26.000000    76.500000  154.0\n",
       "75%            NaN  201541.750000  2015.000000   40.000000   139.250000  154.0\n",
       "max            NaN  201627.000000  2016.000000   53.000000  1147.000000  154.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eachItem2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.02019315],\n",
       "       [0.02194908],\n",
       "       [0.01316945],\n",
       "       [0.04389816],\n",
       "       [0.06848112],\n",
       "       [0.02546093],\n",
       "       [0.02721686],\n",
       "       [0.02985075],\n",
       "       [0.023705  ],\n",
       "       [0.02458297],\n",
       "       [0.05443371],\n",
       "       [0.01316945],\n",
       "       [0.01931519],\n",
       "       [0.28007024],\n",
       "       [0.42054434],\n",
       "       [0.49692713],\n",
       "       [0.79543459],\n",
       "       [0.39508341],\n",
       "       [0.02897278],\n",
       "       [0.04477612],\n",
       "       [0.02546093],\n",
       "       [0.023705  ],\n",
       "       [0.0333626 ],\n",
       "       [0.0500439 ],\n",
       "       [0.04477612],\n",
       "       [0.02809482],\n",
       "       [0.06584723],\n",
       "       [0.06057946],\n",
       "       [0.08867428],\n",
       "       [0.01931519],\n",
       "       [0.03687445],\n",
       "       [0.04389816],\n",
       "       [0.01755926],\n",
       "       [0.01580334],\n",
       "       [0.05355575],\n",
       "       [0.08691835],\n",
       "       [0.06848112],\n",
       "       [0.06848112],\n",
       "       [0.05179982],\n",
       "       [0.07901668],\n",
       "       [0.09130817],\n",
       "       [0.16154522],\n",
       "       [0.19841967],\n",
       "       [0.08604039],\n",
       "       [0.2071993 ],\n",
       "       [0.2572432 ],\n",
       "       [0.33625988],\n",
       "       [0.52238806],\n",
       "       [0.47761194],\n",
       "       [0.05618964],\n",
       "       [0.03599649],\n",
       "       [0.08779631],\n",
       "       [0.05618964],\n",
       "       [0.0834065 ],\n",
       "       [0.05179982],\n",
       "       [0.10272169],\n",
       "       [0.15539947],\n",
       "       [0.03511853],\n",
       "       [0.03775241],\n",
       "       [0.03511853],\n",
       "       [0.05618964],\n",
       "       [0.06409131],\n",
       "       [0.07023705],\n",
       "       [0.04477612],\n",
       "       [0.05179982],\n",
       "       [0.33362599],\n",
       "       [0.43020193],\n",
       "       [0.50219491],\n",
       "       [1.        ],\n",
       "       [0.65408253],\n",
       "       [0.17559263],\n",
       "       [0.08516242],\n",
       "       [0.06935909],\n",
       "       [0.11062335],\n",
       "       [0.10798946],\n",
       "       [0.08955224],\n",
       "       [0.11589113],\n",
       "       [0.05882353],\n",
       "       [0.04741001],\n",
       "       [0.05882353],\n",
       "       [0.0904302 ],\n",
       "       [0.07287094],\n",
       "       [0.04828797],\n",
       "       [0.05531168],\n",
       "       [0.0500439 ],\n",
       "       [0.04565408],\n",
       "       [0.03424056],\n",
       "       [0.04389816],\n",
       "       [0.02546093],\n",
       "       [0.05267779],\n",
       "       [0.11764706],\n",
       "       [0.07726076],\n",
       "       [0.05355575],\n",
       "       [0.08691835],\n",
       "       [0.10096576],\n",
       "       [0.06935909],\n",
       "       [0.04653205],\n",
       "       [0.10623354],\n",
       "       [0.28270413],\n",
       "       [0.42405619],\n",
       "       [0.37576822],\n",
       "       [0.34152766],\n",
       "       [0.11589113],\n",
       "       [0.15803336],\n",
       "       [0.11325724],\n",
       "       [0.09130817],\n",
       "       [0.04214223],\n",
       "       [0.05443371],\n",
       "       [0.1071115 ],\n",
       "       [0.14749781],\n",
       "       [0.05970149],\n",
       "       [0.04565408],\n",
       "       [0.0570676 ],\n",
       "       [0.04741001],\n",
       "       [0.06409131],\n",
       "       [0.06496927],\n",
       "       [0.03160667],\n",
       "       [0.26251097],\n",
       "       [0.30201932],\n",
       "       [0.28182616],\n",
       "       [0.63652327],\n",
       "       [0.40561896],\n",
       "       [0.04389816],\n",
       "       [0.04126427],\n",
       "       [0.03863038],\n",
       "       [0.06584723],\n",
       "       [0.03248464],\n",
       "       [0.05882353],\n",
       "       [0.06584723],\n",
       "       [0.22300263],\n",
       "       [0.1237928 ],\n",
       "       [0.05179982],\n",
       "       [0.06233538],\n",
       "       [0.11852502],\n",
       "       [0.09306409],\n",
       "       [0.04214223],\n",
       "       [0.02546093],\n",
       "       [0.04565408],\n",
       "       [0.04477612],\n",
       "       [0.04302019],\n",
       "       [0.03599649],\n",
       "       [0.03863038],\n",
       "       [0.04389816],\n",
       "       [0.03072871],\n",
       "       [0.0333626 ],\n",
       "       [0.0403863 ],\n",
       "       [0.07813872],\n",
       "       [0.06496927],\n",
       "       [0.03775241],\n",
       "       [0.03775241],\n",
       "       [0.13696225],\n",
       "       [0.22124671],\n",
       "       [0.2238806 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetColumns = [\"QTY\"]\n",
    "targetData = eachItem2[targetColumns]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "qtyList = scaler.fit_transform(targetData)\n",
    "qtyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetColumns = [\"YEARWEEK\",\"QTY\"]\n",
    "\n",
    "targetData = eachItem2[targetColumns]\n",
    "\n",
    "targetData = targetData.set_index(\"YEARWEEK\")\n",
    "\n",
    "qtyList = np.array(targetData.QTY).reshape(-1,1)\n",
    "\n",
    "qtyList.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 데이터 전처리 * 최소 최대값을 구해서 0~1범위로 변환\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "qtyList = scaler.fit_transform(qtyList)\n",
    "\n",
    "stdIndex = int(len(qtyList)*0.7)\n",
    "\n",
    "stdIndex\n",
    "\n",
    "# 데이터 분리\n",
    "train = qtyList[0:stdIndex]\n",
    "test = qtyList[stdIndex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtyList = np.array([100,200,30,20,50,10,20,30,70]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100],\n",
       "       [200],\n",
       "       [ 30],\n",
       "       [ 20],\n",
       "       [ 50],\n",
       "       [ 10],\n",
       "       [ 20],\n",
       "       [ 30],\n",
       "       [ 70]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[100, 200,  30],\n",
       "        [200,  30,  20],\n",
       "        [ 30,  20,  50],\n",
       "        [ 20,  50,  10],\n",
       "        [ 50,  10,  20],\n",
       "        [ 10,  20,  30]]), array([20, 50, 10, 20, 30, 70]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dataset(qtyList, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[100], [200], [30], [20], [50]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        ],\n",
       "        [0.02019315],\n",
       "        [0.02194908],\n",
       "        [0.01316945],\n",
       "        [0.04389816],\n",
       "        [0.06848112],\n",
       "        [0.02546093],\n",
       "        [0.02721686],\n",
       "        [0.02985075],\n",
       "        [0.023705  ]],\n",
       "\n",
       "       [[0.02019315],\n",
       "        [0.02194908],\n",
       "        [0.01316945],\n",
       "        [0.04389816],\n",
       "        [0.06848112],\n",
       "        [0.02546093],\n",
       "        [0.02721686],\n",
       "        [0.02985075],\n",
       "        [0.023705  ],\n",
       "        [0.02458297]],\n",
       "\n",
       "       [[0.02194908],\n",
       "        [0.01316945],\n",
       "        [0.04389816],\n",
       "        [0.06848112],\n",
       "        [0.02546093],\n",
       "        [0.02721686],\n",
       "        [0.02985075],\n",
       "        [0.023705  ],\n",
       "        [0.02458297],\n",
       "        [0.05443371]],\n",
       "\n",
       "       [[0.01316945],\n",
       "        [0.04389816],\n",
       "        [0.06848112],\n",
       "        [0.02546093],\n",
       "        [0.02721686],\n",
       "        [0.02985075],\n",
       "        [0.023705  ],\n",
       "        [0.02458297],\n",
       "        [0.05443371],\n",
       "        [0.01316945]],\n",
       "\n",
       "       [[0.04389816],\n",
       "        [0.06848112],\n",
       "        [0.02546093],\n",
       "        [0.02721686],\n",
       "        [0.02985075],\n",
       "        [0.023705  ],\n",
       "        [0.02458297],\n",
       "        [0.05443371],\n",
       "        [0.01316945],\n",
       "        [0.01931519]],\n",
       "\n",
       "       [[0.06848112],\n",
       "        [0.02546093],\n",
       "        [0.02721686],\n",
       "        [0.02985075],\n",
       "        [0.023705  ],\n",
       "        [0.02458297],\n",
       "        [0.05443371],\n",
       "        [0.01316945],\n",
       "        [0.01931519],\n",
       "        [0.28007024]],\n",
       "\n",
       "       [[0.02546093],\n",
       "        [0.02721686],\n",
       "        [0.02985075],\n",
       "        [0.023705  ],\n",
       "        [0.02458297],\n",
       "        [0.05443371],\n",
       "        [0.01316945],\n",
       "        [0.01931519],\n",
       "        [0.28007024],\n",
       "        [0.42054434]],\n",
       "\n",
       "       [[0.02721686],\n",
       "        [0.02985075],\n",
       "        [0.023705  ],\n",
       "        [0.02458297],\n",
       "        [0.05443371],\n",
       "        [0.01316945],\n",
       "        [0.01931519],\n",
       "        [0.28007024],\n",
       "        [0.42054434],\n",
       "        [0.49692713]],\n",
       "\n",
       "       [[0.02985075],\n",
       "        [0.023705  ],\n",
       "        [0.02458297],\n",
       "        [0.05443371],\n",
       "        [0.01316945],\n",
       "        [0.01931519],\n",
       "        [0.28007024],\n",
       "        [0.42054434],\n",
       "        [0.49692713],\n",
       "        [0.79543459]],\n",
       "\n",
       "       [[0.023705  ],\n",
       "        [0.02458297],\n",
       "        [0.05443371],\n",
       "        [0.01316945],\n",
       "        [0.01931519],\n",
       "        [0.28007024],\n",
       "        [0.42054434],\n",
       "        [0.49692713],\n",
       "        [0.79543459],\n",
       "        [0.39508341]],\n",
       "\n",
       "       [[0.02458297],\n",
       "        [0.05443371],\n",
       "        [0.01316945],\n",
       "        [0.01931519],\n",
       "        [0.28007024],\n",
       "        [0.42054434],\n",
       "        [0.49692713],\n",
       "        [0.79543459],\n",
       "        [0.39508341],\n",
       "        [0.02897278]],\n",
       "\n",
       "       [[0.05443371],\n",
       "        [0.01316945],\n",
       "        [0.01931519],\n",
       "        [0.28007024],\n",
       "        [0.42054434],\n",
       "        [0.49692713],\n",
       "        [0.79543459],\n",
       "        [0.39508341],\n",
       "        [0.02897278],\n",
       "        [0.04477612]],\n",
       "\n",
       "       [[0.01316945],\n",
       "        [0.01931519],\n",
       "        [0.28007024],\n",
       "        [0.42054434],\n",
       "        [0.49692713],\n",
       "        [0.79543459],\n",
       "        [0.39508341],\n",
       "        [0.02897278],\n",
       "        [0.04477612],\n",
       "        [0.02546093]],\n",
       "\n",
       "       [[0.01931519],\n",
       "        [0.28007024],\n",
       "        [0.42054434],\n",
       "        [0.49692713],\n",
       "        [0.79543459],\n",
       "        [0.39508341],\n",
       "        [0.02897278],\n",
       "        [0.04477612],\n",
       "        [0.02546093],\n",
       "        [0.023705  ]],\n",
       "\n",
       "       [[0.28007024],\n",
       "        [0.42054434],\n",
       "        [0.49692713],\n",
       "        [0.79543459],\n",
       "        [0.39508341],\n",
       "        [0.02897278],\n",
       "        [0.04477612],\n",
       "        [0.02546093],\n",
       "        [0.023705  ],\n",
       "        [0.0333626 ]],\n",
       "\n",
       "       [[0.42054434],\n",
       "        [0.49692713],\n",
       "        [0.79543459],\n",
       "        [0.39508341],\n",
       "        [0.02897278],\n",
       "        [0.04477612],\n",
       "        [0.02546093],\n",
       "        [0.023705  ],\n",
       "        [0.0333626 ],\n",
       "        [0.0500439 ]],\n",
       "\n",
       "       [[0.49692713],\n",
       "        [0.79543459],\n",
       "        [0.39508341],\n",
       "        [0.02897278],\n",
       "        [0.04477612],\n",
       "        [0.02546093],\n",
       "        [0.023705  ],\n",
       "        [0.0333626 ],\n",
       "        [0.0500439 ],\n",
       "        [0.04477612]],\n",
       "\n",
       "       [[0.79543459],\n",
       "        [0.39508341],\n",
       "        [0.02897278],\n",
       "        [0.04477612],\n",
       "        [0.02546093],\n",
       "        [0.023705  ],\n",
       "        [0.0333626 ],\n",
       "        [0.0500439 ],\n",
       "        [0.04477612],\n",
       "        [0.02809482]],\n",
       "\n",
       "       [[0.39508341],\n",
       "        [0.02897278],\n",
       "        [0.04477612],\n",
       "        [0.02546093],\n",
       "        [0.023705  ],\n",
       "        [0.0333626 ],\n",
       "        [0.0500439 ],\n",
       "        [0.04477612],\n",
       "        [0.02809482],\n",
       "        [0.06584723]],\n",
       "\n",
       "       [[0.02897278],\n",
       "        [0.04477612],\n",
       "        [0.02546093],\n",
       "        [0.023705  ],\n",
       "        [0.0333626 ],\n",
       "        [0.0500439 ],\n",
       "        [0.04477612],\n",
       "        [0.02809482],\n",
       "        [0.06584723],\n",
       "        [0.06057946]],\n",
       "\n",
       "       [[0.04477612],\n",
       "        [0.02546093],\n",
       "        [0.023705  ],\n",
       "        [0.0333626 ],\n",
       "        [0.0500439 ],\n",
       "        [0.04477612],\n",
       "        [0.02809482],\n",
       "        [0.06584723],\n",
       "        [0.06057946],\n",
       "        [0.08867428]],\n",
       "\n",
       "       [[0.02546093],\n",
       "        [0.023705  ],\n",
       "        [0.0333626 ],\n",
       "        [0.0500439 ],\n",
       "        [0.04477612],\n",
       "        [0.02809482],\n",
       "        [0.06584723],\n",
       "        [0.06057946],\n",
       "        [0.08867428],\n",
       "        [0.01931519]],\n",
       "\n",
       "       [[0.023705  ],\n",
       "        [0.0333626 ],\n",
       "        [0.0500439 ],\n",
       "        [0.04477612],\n",
       "        [0.02809482],\n",
       "        [0.06584723],\n",
       "        [0.06057946],\n",
       "        [0.08867428],\n",
       "        [0.01931519],\n",
       "        [0.03687445]],\n",
       "\n",
       "       [[0.0333626 ],\n",
       "        [0.0500439 ],\n",
       "        [0.04477612],\n",
       "        [0.02809482],\n",
       "        [0.06584723],\n",
       "        [0.06057946],\n",
       "        [0.08867428],\n",
       "        [0.01931519],\n",
       "        [0.03687445],\n",
       "        [0.04389816]],\n",
       "\n",
       "       [[0.0500439 ],\n",
       "        [0.04477612],\n",
       "        [0.02809482],\n",
       "        [0.06584723],\n",
       "        [0.06057946],\n",
       "        [0.08867428],\n",
       "        [0.01931519],\n",
       "        [0.03687445],\n",
       "        [0.04389816],\n",
       "        [0.01755926]],\n",
       "\n",
       "       [[0.04477612],\n",
       "        [0.02809482],\n",
       "        [0.06584723],\n",
       "        [0.06057946],\n",
       "        [0.08867428],\n",
       "        [0.01931519],\n",
       "        [0.03687445],\n",
       "        [0.04389816],\n",
       "        [0.01755926],\n",
       "        [0.01580334]],\n",
       "\n",
       "       [[0.02809482],\n",
       "        [0.06584723],\n",
       "        [0.06057946],\n",
       "        [0.08867428],\n",
       "        [0.01931519],\n",
       "        [0.03687445],\n",
       "        [0.04389816],\n",
       "        [0.01755926],\n",
       "        [0.01580334],\n",
       "        [0.05355575]],\n",
       "\n",
       "       [[0.06584723],\n",
       "        [0.06057946],\n",
       "        [0.08867428],\n",
       "        [0.01931519],\n",
       "        [0.03687445],\n",
       "        [0.04389816],\n",
       "        [0.01755926],\n",
       "        [0.01580334],\n",
       "        [0.05355575],\n",
       "        [0.08691835]],\n",
       "\n",
       "       [[0.06057946],\n",
       "        [0.08867428],\n",
       "        [0.01931519],\n",
       "        [0.03687445],\n",
       "        [0.04389816],\n",
       "        [0.01755926],\n",
       "        [0.01580334],\n",
       "        [0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.06848112]],\n",
       "\n",
       "       [[0.08867428],\n",
       "        [0.01931519],\n",
       "        [0.03687445],\n",
       "        [0.04389816],\n",
       "        [0.01755926],\n",
       "        [0.01580334],\n",
       "        [0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.06848112],\n",
       "        [0.06848112]],\n",
       "\n",
       "       [[0.01931519],\n",
       "        [0.03687445],\n",
       "        [0.04389816],\n",
       "        [0.01755926],\n",
       "        [0.01580334],\n",
       "        [0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.06848112],\n",
       "        [0.06848112],\n",
       "        [0.05179982]],\n",
       "\n",
       "       [[0.03687445],\n",
       "        [0.04389816],\n",
       "        [0.01755926],\n",
       "        [0.01580334],\n",
       "        [0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.06848112],\n",
       "        [0.06848112],\n",
       "        [0.05179982],\n",
       "        [0.07901668]],\n",
       "\n",
       "       [[0.04389816],\n",
       "        [0.01755926],\n",
       "        [0.01580334],\n",
       "        [0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.06848112],\n",
       "        [0.06848112],\n",
       "        [0.05179982],\n",
       "        [0.07901668],\n",
       "        [0.09130817]],\n",
       "\n",
       "       [[0.01755926],\n",
       "        [0.01580334],\n",
       "        [0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.06848112],\n",
       "        [0.06848112],\n",
       "        [0.05179982],\n",
       "        [0.07901668],\n",
       "        [0.09130817],\n",
       "        [0.16154522]],\n",
       "\n",
       "       [[0.01580334],\n",
       "        [0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.06848112],\n",
       "        [0.06848112],\n",
       "        [0.05179982],\n",
       "        [0.07901668],\n",
       "        [0.09130817],\n",
       "        [0.16154522],\n",
       "        [0.19841967]],\n",
       "\n",
       "       [[0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.06848112],\n",
       "        [0.06848112],\n",
       "        [0.05179982],\n",
       "        [0.07901668],\n",
       "        [0.09130817],\n",
       "        [0.16154522],\n",
       "        [0.19841967],\n",
       "        [0.08604039]],\n",
       "\n",
       "       [[0.08691835],\n",
       "        [0.06848112],\n",
       "        [0.06848112],\n",
       "        [0.05179982],\n",
       "        [0.07901668],\n",
       "        [0.09130817],\n",
       "        [0.16154522],\n",
       "        [0.19841967],\n",
       "        [0.08604039],\n",
       "        [0.2071993 ]],\n",
       "\n",
       "       [[0.06848112],\n",
       "        [0.06848112],\n",
       "        [0.05179982],\n",
       "        [0.07901668],\n",
       "        [0.09130817],\n",
       "        [0.16154522],\n",
       "        [0.19841967],\n",
       "        [0.08604039],\n",
       "        [0.2071993 ],\n",
       "        [0.2572432 ]],\n",
       "\n",
       "       [[0.06848112],\n",
       "        [0.05179982],\n",
       "        [0.07901668],\n",
       "        [0.09130817],\n",
       "        [0.16154522],\n",
       "        [0.19841967],\n",
       "        [0.08604039],\n",
       "        [0.2071993 ],\n",
       "        [0.2572432 ],\n",
       "        [0.33625988]],\n",
       "\n",
       "       [[0.05179982],\n",
       "        [0.07901668],\n",
       "        [0.09130817],\n",
       "        [0.16154522],\n",
       "        [0.19841967],\n",
       "        [0.08604039],\n",
       "        [0.2071993 ],\n",
       "        [0.2572432 ],\n",
       "        [0.33625988],\n",
       "        [0.52238806]],\n",
       "\n",
       "       [[0.07901668],\n",
       "        [0.09130817],\n",
       "        [0.16154522],\n",
       "        [0.19841967],\n",
       "        [0.08604039],\n",
       "        [0.2071993 ],\n",
       "        [0.2572432 ],\n",
       "        [0.33625988],\n",
       "        [0.52238806],\n",
       "        [0.47761194]],\n",
       "\n",
       "       [[0.09130817],\n",
       "        [0.16154522],\n",
       "        [0.19841967],\n",
       "        [0.08604039],\n",
       "        [0.2071993 ],\n",
       "        [0.2572432 ],\n",
       "        [0.33625988],\n",
       "        [0.52238806],\n",
       "        [0.47761194],\n",
       "        [0.05618964]],\n",
       "\n",
       "       [[0.16154522],\n",
       "        [0.19841967],\n",
       "        [0.08604039],\n",
       "        [0.2071993 ],\n",
       "        [0.2572432 ],\n",
       "        [0.33625988],\n",
       "        [0.52238806],\n",
       "        [0.47761194],\n",
       "        [0.05618964],\n",
       "        [0.03599649]],\n",
       "\n",
       "       [[0.19841967],\n",
       "        [0.08604039],\n",
       "        [0.2071993 ],\n",
       "        [0.2572432 ],\n",
       "        [0.33625988],\n",
       "        [0.52238806],\n",
       "        [0.47761194],\n",
       "        [0.05618964],\n",
       "        [0.03599649],\n",
       "        [0.08779631]],\n",
       "\n",
       "       [[0.08604039],\n",
       "        [0.2071993 ],\n",
       "        [0.2572432 ],\n",
       "        [0.33625988],\n",
       "        [0.52238806],\n",
       "        [0.47761194],\n",
       "        [0.05618964],\n",
       "        [0.03599649],\n",
       "        [0.08779631],\n",
       "        [0.05618964]],\n",
       "\n",
       "       [[0.2071993 ],\n",
       "        [0.2572432 ],\n",
       "        [0.33625988],\n",
       "        [0.52238806],\n",
       "        [0.47761194],\n",
       "        [0.05618964],\n",
       "        [0.03599649],\n",
       "        [0.08779631],\n",
       "        [0.05618964],\n",
       "        [0.0834065 ]],\n",
       "\n",
       "       [[0.2572432 ],\n",
       "        [0.33625988],\n",
       "        [0.52238806],\n",
       "        [0.47761194],\n",
       "        [0.05618964],\n",
       "        [0.03599649],\n",
       "        [0.08779631],\n",
       "        [0.05618964],\n",
       "        [0.0834065 ],\n",
       "        [0.05179982]],\n",
       "\n",
       "       [[0.33625988],\n",
       "        [0.52238806],\n",
       "        [0.47761194],\n",
       "        [0.05618964],\n",
       "        [0.03599649],\n",
       "        [0.08779631],\n",
       "        [0.05618964],\n",
       "        [0.0834065 ],\n",
       "        [0.05179982],\n",
       "        [0.10272169]],\n",
       "\n",
       "       [[0.52238806],\n",
       "        [0.47761194],\n",
       "        [0.05618964],\n",
       "        [0.03599649],\n",
       "        [0.08779631],\n",
       "        [0.05618964],\n",
       "        [0.0834065 ],\n",
       "        [0.05179982],\n",
       "        [0.10272169],\n",
       "        [0.15539947]],\n",
       "\n",
       "       [[0.47761194],\n",
       "        [0.05618964],\n",
       "        [0.03599649],\n",
       "        [0.08779631],\n",
       "        [0.05618964],\n",
       "        [0.0834065 ],\n",
       "        [0.05179982],\n",
       "        [0.10272169],\n",
       "        [0.15539947],\n",
       "        [0.03511853]],\n",
       "\n",
       "       [[0.05618964],\n",
       "        [0.03599649],\n",
       "        [0.08779631],\n",
       "        [0.05618964],\n",
       "        [0.0834065 ],\n",
       "        [0.05179982],\n",
       "        [0.10272169],\n",
       "        [0.15539947],\n",
       "        [0.03511853],\n",
       "        [0.03775241]],\n",
       "\n",
       "       [[0.03599649],\n",
       "        [0.08779631],\n",
       "        [0.05618964],\n",
       "        [0.0834065 ],\n",
       "        [0.05179982],\n",
       "        [0.10272169],\n",
       "        [0.15539947],\n",
       "        [0.03511853],\n",
       "        [0.03775241],\n",
       "        [0.03511853]],\n",
       "\n",
       "       [[0.08779631],\n",
       "        [0.05618964],\n",
       "        [0.0834065 ],\n",
       "        [0.05179982],\n",
       "        [0.10272169],\n",
       "        [0.15539947],\n",
       "        [0.03511853],\n",
       "        [0.03775241],\n",
       "        [0.03511853],\n",
       "        [0.05618964]],\n",
       "\n",
       "       [[0.05618964],\n",
       "        [0.0834065 ],\n",
       "        [0.05179982],\n",
       "        [0.10272169],\n",
       "        [0.15539947],\n",
       "        [0.03511853],\n",
       "        [0.03775241],\n",
       "        [0.03511853],\n",
       "        [0.05618964],\n",
       "        [0.06409131]],\n",
       "\n",
       "       [[0.0834065 ],\n",
       "        [0.05179982],\n",
       "        [0.10272169],\n",
       "        [0.15539947],\n",
       "        [0.03511853],\n",
       "        [0.03775241],\n",
       "        [0.03511853],\n",
       "        [0.05618964],\n",
       "        [0.06409131],\n",
       "        [0.07023705]],\n",
       "\n",
       "       [[0.05179982],\n",
       "        [0.10272169],\n",
       "        [0.15539947],\n",
       "        [0.03511853],\n",
       "        [0.03775241],\n",
       "        [0.03511853],\n",
       "        [0.05618964],\n",
       "        [0.06409131],\n",
       "        [0.07023705],\n",
       "        [0.04477612]],\n",
       "\n",
       "       [[0.10272169],\n",
       "        [0.15539947],\n",
       "        [0.03511853],\n",
       "        [0.03775241],\n",
       "        [0.03511853],\n",
       "        [0.05618964],\n",
       "        [0.06409131],\n",
       "        [0.07023705],\n",
       "        [0.04477612],\n",
       "        [0.05179982]],\n",
       "\n",
       "       [[0.15539947],\n",
       "        [0.03511853],\n",
       "        [0.03775241],\n",
       "        [0.03511853],\n",
       "        [0.05618964],\n",
       "        [0.06409131],\n",
       "        [0.07023705],\n",
       "        [0.04477612],\n",
       "        [0.05179982],\n",
       "        [0.33362599]],\n",
       "\n",
       "       [[0.03511853],\n",
       "        [0.03775241],\n",
       "        [0.03511853],\n",
       "        [0.05618964],\n",
       "        [0.06409131],\n",
       "        [0.07023705],\n",
       "        [0.04477612],\n",
       "        [0.05179982],\n",
       "        [0.33362599],\n",
       "        [0.43020193]],\n",
       "\n",
       "       [[0.03775241],\n",
       "        [0.03511853],\n",
       "        [0.05618964],\n",
       "        [0.06409131],\n",
       "        [0.07023705],\n",
       "        [0.04477612],\n",
       "        [0.05179982],\n",
       "        [0.33362599],\n",
       "        [0.43020193],\n",
       "        [0.50219491]],\n",
       "\n",
       "       [[0.03511853],\n",
       "        [0.05618964],\n",
       "        [0.06409131],\n",
       "        [0.07023705],\n",
       "        [0.04477612],\n",
       "        [0.05179982],\n",
       "        [0.33362599],\n",
       "        [0.43020193],\n",
       "        [0.50219491],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.05618964],\n",
       "        [0.06409131],\n",
       "        [0.07023705],\n",
       "        [0.04477612],\n",
       "        [0.05179982],\n",
       "        [0.33362599],\n",
       "        [0.43020193],\n",
       "        [0.50219491],\n",
       "        [1.        ],\n",
       "        [0.65408253]],\n",
       "\n",
       "       [[0.06409131],\n",
       "        [0.07023705],\n",
       "        [0.04477612],\n",
       "        [0.05179982],\n",
       "        [0.33362599],\n",
       "        [0.43020193],\n",
       "        [0.50219491],\n",
       "        [1.        ],\n",
       "        [0.65408253],\n",
       "        [0.17559263]],\n",
       "\n",
       "       [[0.07023705],\n",
       "        [0.04477612],\n",
       "        [0.05179982],\n",
       "        [0.33362599],\n",
       "        [0.43020193],\n",
       "        [0.50219491],\n",
       "        [1.        ],\n",
       "        [0.65408253],\n",
       "        [0.17559263],\n",
       "        [0.08516242]],\n",
       "\n",
       "       [[0.04477612],\n",
       "        [0.05179982],\n",
       "        [0.33362599],\n",
       "        [0.43020193],\n",
       "        [0.50219491],\n",
       "        [1.        ],\n",
       "        [0.65408253],\n",
       "        [0.17559263],\n",
       "        [0.08516242],\n",
       "        [0.06935909]],\n",
       "\n",
       "       [[0.05179982],\n",
       "        [0.33362599],\n",
       "        [0.43020193],\n",
       "        [0.50219491],\n",
       "        [1.        ],\n",
       "        [0.65408253],\n",
       "        [0.17559263],\n",
       "        [0.08516242],\n",
       "        [0.06935909],\n",
       "        [0.11062335]],\n",
       "\n",
       "       [[0.33362599],\n",
       "        [0.43020193],\n",
       "        [0.50219491],\n",
       "        [1.        ],\n",
       "        [0.65408253],\n",
       "        [0.17559263],\n",
       "        [0.08516242],\n",
       "        [0.06935909],\n",
       "        [0.11062335],\n",
       "        [0.10798946]],\n",
       "\n",
       "       [[0.43020193],\n",
       "        [0.50219491],\n",
       "        [1.        ],\n",
       "        [0.65408253],\n",
       "        [0.17559263],\n",
       "        [0.08516242],\n",
       "        [0.06935909],\n",
       "        [0.11062335],\n",
       "        [0.10798946],\n",
       "        [0.08955224]],\n",
       "\n",
       "       [[0.50219491],\n",
       "        [1.        ],\n",
       "        [0.65408253],\n",
       "        [0.17559263],\n",
       "        [0.08516242],\n",
       "        [0.06935909],\n",
       "        [0.11062335],\n",
       "        [0.10798946],\n",
       "        [0.08955224],\n",
       "        [0.11589113]],\n",
       "\n",
       "       [[1.        ],\n",
       "        [0.65408253],\n",
       "        [0.17559263],\n",
       "        [0.08516242],\n",
       "        [0.06935909],\n",
       "        [0.11062335],\n",
       "        [0.10798946],\n",
       "        [0.08955224],\n",
       "        [0.11589113],\n",
       "        [0.05882353]],\n",
       "\n",
       "       [[0.65408253],\n",
       "        [0.17559263],\n",
       "        [0.08516242],\n",
       "        [0.06935909],\n",
       "        [0.11062335],\n",
       "        [0.10798946],\n",
       "        [0.08955224],\n",
       "        [0.11589113],\n",
       "        [0.05882353],\n",
       "        [0.04741001]],\n",
       "\n",
       "       [[0.17559263],\n",
       "        [0.08516242],\n",
       "        [0.06935909],\n",
       "        [0.11062335],\n",
       "        [0.10798946],\n",
       "        [0.08955224],\n",
       "        [0.11589113],\n",
       "        [0.05882353],\n",
       "        [0.04741001],\n",
       "        [0.05882353]],\n",
       "\n",
       "       [[0.08516242],\n",
       "        [0.06935909],\n",
       "        [0.11062335],\n",
       "        [0.10798946],\n",
       "        [0.08955224],\n",
       "        [0.11589113],\n",
       "        [0.05882353],\n",
       "        [0.04741001],\n",
       "        [0.05882353],\n",
       "        [0.0904302 ]],\n",
       "\n",
       "       [[0.06935909],\n",
       "        [0.11062335],\n",
       "        [0.10798946],\n",
       "        [0.08955224],\n",
       "        [0.11589113],\n",
       "        [0.05882353],\n",
       "        [0.04741001],\n",
       "        [0.05882353],\n",
       "        [0.0904302 ],\n",
       "        [0.07287094]],\n",
       "\n",
       "       [[0.11062335],\n",
       "        [0.10798946],\n",
       "        [0.08955224],\n",
       "        [0.11589113],\n",
       "        [0.05882353],\n",
       "        [0.04741001],\n",
       "        [0.05882353],\n",
       "        [0.0904302 ],\n",
       "        [0.07287094],\n",
       "        [0.04828797]],\n",
       "\n",
       "       [[0.10798946],\n",
       "        [0.08955224],\n",
       "        [0.11589113],\n",
       "        [0.05882353],\n",
       "        [0.04741001],\n",
       "        [0.05882353],\n",
       "        [0.0904302 ],\n",
       "        [0.07287094],\n",
       "        [0.04828797],\n",
       "        [0.05531168]],\n",
       "\n",
       "       [[0.08955224],\n",
       "        [0.11589113],\n",
       "        [0.05882353],\n",
       "        [0.04741001],\n",
       "        [0.05882353],\n",
       "        [0.0904302 ],\n",
       "        [0.07287094],\n",
       "        [0.04828797],\n",
       "        [0.05531168],\n",
       "        [0.0500439 ]],\n",
       "\n",
       "       [[0.11589113],\n",
       "        [0.05882353],\n",
       "        [0.04741001],\n",
       "        [0.05882353],\n",
       "        [0.0904302 ],\n",
       "        [0.07287094],\n",
       "        [0.04828797],\n",
       "        [0.05531168],\n",
       "        [0.0500439 ],\n",
       "        [0.04565408]],\n",
       "\n",
       "       [[0.05882353],\n",
       "        [0.04741001],\n",
       "        [0.05882353],\n",
       "        [0.0904302 ],\n",
       "        [0.07287094],\n",
       "        [0.04828797],\n",
       "        [0.05531168],\n",
       "        [0.0500439 ],\n",
       "        [0.04565408],\n",
       "        [0.03424056]],\n",
       "\n",
       "       [[0.04741001],\n",
       "        [0.05882353],\n",
       "        [0.0904302 ],\n",
       "        [0.07287094],\n",
       "        [0.04828797],\n",
       "        [0.05531168],\n",
       "        [0.0500439 ],\n",
       "        [0.04565408],\n",
       "        [0.03424056],\n",
       "        [0.04389816]],\n",
       "\n",
       "       [[0.05882353],\n",
       "        [0.0904302 ],\n",
       "        [0.07287094],\n",
       "        [0.04828797],\n",
       "        [0.05531168],\n",
       "        [0.0500439 ],\n",
       "        [0.04565408],\n",
       "        [0.03424056],\n",
       "        [0.04389816],\n",
       "        [0.02546093]],\n",
       "\n",
       "       [[0.0904302 ],\n",
       "        [0.07287094],\n",
       "        [0.04828797],\n",
       "        [0.05531168],\n",
       "        [0.0500439 ],\n",
       "        [0.04565408],\n",
       "        [0.03424056],\n",
       "        [0.04389816],\n",
       "        [0.02546093],\n",
       "        [0.05267779]],\n",
       "\n",
       "       [[0.07287094],\n",
       "        [0.04828797],\n",
       "        [0.05531168],\n",
       "        [0.0500439 ],\n",
       "        [0.04565408],\n",
       "        [0.03424056],\n",
       "        [0.04389816],\n",
       "        [0.02546093],\n",
       "        [0.05267779],\n",
       "        [0.11764706]],\n",
       "\n",
       "       [[0.04828797],\n",
       "        [0.05531168],\n",
       "        [0.0500439 ],\n",
       "        [0.04565408],\n",
       "        [0.03424056],\n",
       "        [0.04389816],\n",
       "        [0.02546093],\n",
       "        [0.05267779],\n",
       "        [0.11764706],\n",
       "        [0.07726076]],\n",
       "\n",
       "       [[0.05531168],\n",
       "        [0.0500439 ],\n",
       "        [0.04565408],\n",
       "        [0.03424056],\n",
       "        [0.04389816],\n",
       "        [0.02546093],\n",
       "        [0.05267779],\n",
       "        [0.11764706],\n",
       "        [0.07726076],\n",
       "        [0.05355575]],\n",
       "\n",
       "       [[0.0500439 ],\n",
       "        [0.04565408],\n",
       "        [0.03424056],\n",
       "        [0.04389816],\n",
       "        [0.02546093],\n",
       "        [0.05267779],\n",
       "        [0.11764706],\n",
       "        [0.07726076],\n",
       "        [0.05355575],\n",
       "        [0.08691835]],\n",
       "\n",
       "       [[0.04565408],\n",
       "        [0.03424056],\n",
       "        [0.04389816],\n",
       "        [0.02546093],\n",
       "        [0.05267779],\n",
       "        [0.11764706],\n",
       "        [0.07726076],\n",
       "        [0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.10096576]],\n",
       "\n",
       "       [[0.03424056],\n",
       "        [0.04389816],\n",
       "        [0.02546093],\n",
       "        [0.05267779],\n",
       "        [0.11764706],\n",
       "        [0.07726076],\n",
       "        [0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.10096576],\n",
       "        [0.06935909]],\n",
       "\n",
       "       [[0.04389816],\n",
       "        [0.02546093],\n",
       "        [0.05267779],\n",
       "        [0.11764706],\n",
       "        [0.07726076],\n",
       "        [0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.10096576],\n",
       "        [0.06935909],\n",
       "        [0.04653205]],\n",
       "\n",
       "       [[0.02546093],\n",
       "        [0.05267779],\n",
       "        [0.11764706],\n",
       "        [0.07726076],\n",
       "        [0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.10096576],\n",
       "        [0.06935909],\n",
       "        [0.04653205],\n",
       "        [0.10623354]],\n",
       "\n",
       "       [[0.05267779],\n",
       "        [0.11764706],\n",
       "        [0.07726076],\n",
       "        [0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.10096576],\n",
       "        [0.06935909],\n",
       "        [0.04653205],\n",
       "        [0.10623354],\n",
       "        [0.28270413]],\n",
       "\n",
       "       [[0.11764706],\n",
       "        [0.07726076],\n",
       "        [0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.10096576],\n",
       "        [0.06935909],\n",
       "        [0.04653205],\n",
       "        [0.10623354],\n",
       "        [0.28270413],\n",
       "        [0.42405619]],\n",
       "\n",
       "       [[0.07726076],\n",
       "        [0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.10096576],\n",
       "        [0.06935909],\n",
       "        [0.04653205],\n",
       "        [0.10623354],\n",
       "        [0.28270413],\n",
       "        [0.42405619],\n",
       "        [0.37576822]],\n",
       "\n",
       "       [[0.05355575],\n",
       "        [0.08691835],\n",
       "        [0.10096576],\n",
       "        [0.06935909],\n",
       "        [0.04653205],\n",
       "        [0.10623354],\n",
       "        [0.28270413],\n",
       "        [0.42405619],\n",
       "        [0.37576822],\n",
       "        [0.34152766]],\n",
       "\n",
       "       [[0.08691835],\n",
       "        [0.10096576],\n",
       "        [0.06935909],\n",
       "        [0.04653205],\n",
       "        [0.10623354],\n",
       "        [0.28270413],\n",
       "        [0.42405619],\n",
       "        [0.37576822],\n",
       "        [0.34152766],\n",
       "        [0.11589113]],\n",
       "\n",
       "       [[0.10096576],\n",
       "        [0.06935909],\n",
       "        [0.04653205],\n",
       "        [0.10623354],\n",
       "        [0.28270413],\n",
       "        [0.42405619],\n",
       "        [0.37576822],\n",
       "        [0.34152766],\n",
       "        [0.11589113],\n",
       "        [0.15803336]],\n",
       "\n",
       "       [[0.06935909],\n",
       "        [0.04653205],\n",
       "        [0.10623354],\n",
       "        [0.28270413],\n",
       "        [0.42405619],\n",
       "        [0.37576822],\n",
       "        [0.34152766],\n",
       "        [0.11589113],\n",
       "        [0.15803336],\n",
       "        [0.11325724]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "x_train, y_train = create_dataset(train, look_back)\n",
    "x_test, y_test = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 10)\n",
      "(37, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리\n",
    "x_train = x_train.reshape( len(x_train), look_back, 1)\n",
    "x_test = x_test.reshape( len(x_test), look_back, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputShape = x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputShape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 생성 (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=inputShape))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 97 samples, validate on 37 samples\n",
      "Epoch 1/50\n",
      "97/97 [==============================] - 1s 7ms/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 2/50\n",
      "97/97 [==============================] - 0s 253us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 3/50\n",
      "97/97 [==============================] - 0s 195us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 4/50\n",
      "97/97 [==============================] - 0s 206us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 5/50\n",
      "97/97 [==============================] - 0s 218us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 6/50\n",
      "97/97 [==============================] - 0s 236us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 7/50\n",
      "97/97 [==============================] - 0s 226us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 8/50\n",
      "97/97 [==============================] - 0s 175us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 9/50\n",
      "97/97 [==============================] - 0s 185us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 10/50\n",
      "97/97 [==============================] - 0s 185us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 11/50\n",
      "97/97 [==============================] - 0s 175us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 12/50\n",
      "97/97 [==============================] - 0s 185us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 13/50\n",
      "97/97 [==============================] - 0s 175us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 14/50\n",
      "97/97 [==============================] - 0s 185us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 15/50\n",
      "97/97 [==============================] - 0s 185us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 16/50\n",
      "97/97 [==============================] - 0s 185us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 17/50\n",
      "97/97 [==============================] - 0s 175us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 18/50\n",
      "97/97 [==============================] - 0s 195us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 19/50\n",
      "97/97 [==============================] - 0s 165us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 20/50\n",
      "97/97 [==============================] - 0s 216us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 21/50\n",
      "97/97 [==============================] - 0s 278us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 22/50\n",
      "97/97 [==============================] - 0s 195us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 23/50\n",
      "97/97 [==============================] - 0s 175us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 24/50\n",
      "97/97 [==============================] - 0s 195us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 25/50\n",
      "97/97 [==============================] - 0s 278us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 26/50\n",
      "97/97 [==============================] - 0s 195us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 27/50\n",
      "97/97 [==============================] - 0s 206us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 28/50\n",
      "97/97 [==============================] - 0s 185us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 29/50\n",
      "97/97 [==============================] - 0s 165us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 30/50\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 31/50\n",
      "97/97 [==============================] - 0s 206us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 32/50\n",
      "97/97 [==============================] - 0s 185us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 33/50\n",
      "97/97 [==============================] - 0s 175us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 34/50\n",
      "97/97 [==============================] - 0s 195us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 35/50\n",
      "97/97 [==============================] - 0s 185us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 36/50\n",
      "97/97 [==============================] - 0s 206us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 37/50\n",
      "97/97 [==============================] - 0s 288us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 38/50\n",
      "97/97 [==============================] - 0s 195us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 39/50\n",
      "97/97 [==============================] - 0s 185us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 40/50\n",
      "97/97 [==============================] - 0s 391us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 41/50\n",
      "97/97 [==============================] - 0s 206us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 42/50\n",
      "97/97 [==============================] - 0s 175us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 43/50\n",
      "97/97 [==============================] - 0s 164us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 44/50\n",
      "97/97 [==============================] - 0s 195us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 45/50\n",
      "97/97 [==============================] - 0s 195us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 46/50\n",
      "97/97 [==============================] - 0s 185us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 47/50\n",
      "97/97 [==============================] - 0s 350us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 48/50\n",
      "97/97 [==============================] - 0s 216us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 49/50\n",
      "97/97 [==============================] - 0s 278us/sample - loss: 0.0513 - val_loss: 0.0289\n",
      "Epoch 50/50\n",
      "97/97 [==============================] - 0s 206us/sample - loss: 0.0513 - val_loss: 0.0289\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  0.05132901156801231\n",
      "Test Score:  0.028936740866786725\n"
     ]
    }
   ],
   "source": [
    "trainScore = model.evaluate(x_train, y_train, verbose=0)\n",
    "model.reset_states()\n",
    "print('Train Score: ', trainScore)\n",
    "\n",
    "testScore = model.evaluate(x_test, y_test, verbose=0)\n",
    "model.reset_states()\n",
    "print('Test Score: ', testScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0   8.0\n",
       "1   8.0\n",
       "2   8.0\n",
       "3   8.0\n",
       "4   8.0\n",
       "5   8.0\n",
       "6   8.0\n",
       "7   8.0\n",
       "8   8.0\n",
       "9   8.0\n",
       "10  8.0\n",
       "11  8.0\n",
       "12  8.0\n",
       "13  8.0\n",
       "14  8.0\n",
       "15  8.0\n",
       "16  8.0\n",
       "17  8.0\n",
       "18  8.0\n",
       "19  8.0\n",
       "20  8.0\n",
       "21  8.0\n",
       "22  8.0\n",
       "23  8.0\n",
       "24  8.0\n",
       "25  8.0\n",
       "26  8.0\n",
       "27  8.0\n",
       "28  8.0\n",
       "29  8.0\n",
       "30  8.0\n",
       "31  8.0\n",
       "32  8.0\n",
       "33  8.0\n",
       "34  8.0\n",
       "35  8.0\n",
       "36  8.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.predict(x_test[0].reshape(1, 10))\n",
    "\n",
    "pred = pd.DataFrame(model.predict(x_test))\n",
    "\n",
    "pd.DataFrame(scaler.inverse_transform(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 예측 (1 주)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currQty = x_test[-1:]\n",
    "\n",
    "currQty = currQty.flatten()\n",
    "\n",
    "len(currQty)\n",
    "\n",
    "currQty = currQty.reshape(-1,1)\n",
    "\n",
    "predictValue = model.predict(currQty.reshape(1,look_back,1))\n",
    "predictValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "inversePred = scaler.inverse_transform(predictValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 예측 (연속 주)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtyIn = list(test.flatten()[-10:])\n",
    "\n",
    "pred_count = 30 # 최대 예측 개수 정의\n",
    "\n",
    "pred_out = []\n",
    "\n",
    "for i in range(pred_count):\n",
    "    sample_in = np.array(qtyIn)\n",
    "    currQty = sample_in.reshape(-1,1)\n",
    "    predValue = model.predict(currQty.reshape(1,look_back,1)).flatten()[0]\n",
    "    pred_out.append(predValue)\n",
    "    qtyIn.append(predValue)\n",
    "    qtyIn.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 생성 (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=inputShape))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 97 samples, validate on 37 samples\n",
      "Epoch 1/200\n",
      "97/97 [==============================] - 2s 18ms/sample - loss: 0.0501 - val_loss: 0.0225\n",
      "Epoch 2/200\n",
      "97/97 [==============================] - 0s 708us/sample - loss: 0.0405 - val_loss: 0.0182\n",
      "Epoch 3/200\n",
      "97/97 [==============================] - 0s 421us/sample - loss: 0.0348 - val_loss: 0.0167\n",
      "Epoch 4/200\n",
      "97/97 [==============================] - 0s 535us/sample - loss: 0.0328 - val_loss: 0.0168\n",
      "Epoch 5/200\n",
      "97/97 [==============================] - 0s 514us/sample - loss: 0.0314 - val_loss: 0.0172\n",
      "Epoch 6/200\n",
      "97/97 [==============================] - 0s 411us/sample - loss: 0.0332 - val_loss: 0.0171\n",
      "Epoch 7/200\n",
      "97/97 [==============================] - 0s 442us/sample - loss: 0.0309 - val_loss: 0.0170\n",
      "Epoch 8/200\n",
      "97/97 [==============================] - 0s 298us/sample - loss: 0.0309 - val_loss: 0.0169\n",
      "Epoch 9/200\n",
      "97/97 [==============================] - 0s 350us/sample - loss: 0.0333 - val_loss: 0.0163\n",
      "Epoch 10/200\n",
      "97/97 [==============================] - 0s 452us/sample - loss: 0.0314 - val_loss: 0.0159\n",
      "Epoch 11/200\n",
      "97/97 [==============================] - 0s 298us/sample - loss: 0.0301 - val_loss: 0.0157\n",
      "Epoch 12/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0295 - val_loss: 0.0155\n",
      "Epoch 13/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0293 - val_loss: 0.0154\n",
      "Epoch 14/200\n",
      "97/97 [==============================] - 0s 504us/sample - loss: 0.0299 - val_loss: 0.0153\n",
      "Epoch 15/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0304 - val_loss: 0.0153\n",
      "Epoch 16/200\n",
      "97/97 [==============================] - 0s 308us/sample - loss: 0.0300 - val_loss: 0.0152\n",
      "Epoch 17/200\n",
      "97/97 [==============================] - 0s 350us/sample - loss: 0.0286 - val_loss: 0.0151\n",
      "Epoch 18/200\n",
      "97/97 [==============================] - 0s 308us/sample - loss: 0.0309 - val_loss: 0.0150\n",
      "Epoch 19/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0307 - val_loss: 0.0150\n",
      "Epoch 20/200\n",
      "97/97 [==============================] - 0s 308us/sample - loss: 0.0272 - val_loss: 0.0151\n",
      "Epoch 21/200\n",
      "97/97 [==============================] - 0s 308us/sample - loss: 0.0288 - val_loss: 0.0152\n",
      "Epoch 22/200\n",
      "97/97 [==============================] - 0s 350us/sample - loss: 0.0288 - val_loss: 0.0149\n",
      "Epoch 23/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0274 - val_loss: 0.0150\n",
      "Epoch 24/200\n",
      "97/97 [==============================] - 0s 401us/sample - loss: 0.0265 - val_loss: 0.0162\n",
      "Epoch 25/200\n",
      "97/97 [==============================] - 0s 370us/sample - loss: 0.0280 - val_loss: 0.0184\n",
      "Epoch 26/200\n",
      "97/97 [==============================] - 0s 380us/sample - loss: 0.0273 - val_loss: 0.0184\n",
      "Epoch 27/200\n",
      "97/97 [==============================] - 0s 391us/sample - loss: 0.0268 - val_loss: 0.0169\n",
      "Epoch 28/200\n",
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0254 - val_loss: 0.0152\n",
      "Epoch 29/200\n",
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0245 - val_loss: 0.0143\n",
      "Epoch 30/200\n",
      "97/97 [==============================] - 0s 391us/sample - loss: 0.0262 - val_loss: 0.0139\n",
      "Epoch 31/200\n",
      "97/97 [==============================] - 0s 421us/sample - loss: 0.0249 - val_loss: 0.0139\n",
      "Epoch 32/200\n",
      "97/97 [==============================] - 0s 350us/sample - loss: 0.0242 - val_loss: 0.0138\n",
      "Epoch 33/200\n",
      "97/97 [==============================] - 0s 350us/sample - loss: 0.0257 - val_loss: 0.0139\n",
      "Epoch 34/200\n",
      "97/97 [==============================] - 0s 514us/sample - loss: 0.0244 - val_loss: 0.0144\n",
      "Epoch 35/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0245 - val_loss: 0.0147\n",
      "Epoch 36/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0249 - val_loss: 0.0147\n",
      "Epoch 37/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0244 - val_loss: 0.0145\n",
      "Epoch 38/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0250 - val_loss: 0.0140\n",
      "Epoch 39/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0235 - val_loss: 0.0137\n",
      "Epoch 40/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0247 - val_loss: 0.0135\n",
      "Epoch 41/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0243 - val_loss: 0.0134\n",
      "Epoch 42/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0234 - val_loss: 0.0134\n",
      "Epoch 43/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0241 - val_loss: 0.0134\n",
      "Epoch 44/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0240 - val_loss: 0.0138\n",
      "Epoch 45/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0242 - val_loss: 0.0148\n",
      "Epoch 46/200\n",
      "97/97 [==============================] - 0s 370us/sample - loss: 0.0205 - val_loss: 0.0151\n",
      "Epoch 47/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0232 - val_loss: 0.0144\n",
      "Epoch 48/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0252 - val_loss: 0.0138\n",
      "Epoch 49/200\n",
      "97/97 [==============================] - 0s 370us/sample - loss: 0.0227 - val_loss: 0.0143\n",
      "Epoch 50/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0229 - val_loss: 0.0143\n",
      "Epoch 51/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0227 - val_loss: 0.0135\n",
      "Epoch 52/200\n",
      "97/97 [==============================] - 0s 350us/sample - loss: 0.0237 - val_loss: 0.0130\n",
      "Epoch 53/200\n",
      "97/97 [==============================] - 0s 308us/sample - loss: 0.0227 - val_loss: 0.0129\n",
      "Epoch 54/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0210 - val_loss: 0.0129\n",
      "Epoch 55/200\n",
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0239 - val_loss: 0.0128\n",
      "Epoch 56/200\n",
      "97/97 [==============================] - 0s 298us/sample - loss: 0.0217 - val_loss: 0.0129\n",
      "Epoch 57/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0234 - val_loss: 0.0132\n",
      "Epoch 58/200\n",
      "97/97 [==============================] - 0s 370us/sample - loss: 0.0224 - val_loss: 0.0133\n",
      "Epoch 59/200\n",
      "97/97 [==============================] - 0s 391us/sample - loss: 0.0236 - val_loss: 0.0132\n",
      "Epoch 60/200\n",
      "97/97 [==============================] - 0s 391us/sample - loss: 0.0192 - val_loss: 0.0130\n",
      "Epoch 61/200\n",
      "97/97 [==============================] - 0s 401us/sample - loss: 0.0221 - val_loss: 0.0133\n",
      "Epoch 62/200\n",
      "97/97 [==============================] - 0s 391us/sample - loss: 0.0219 - val_loss: 0.0142\n",
      "Epoch 63/200\n",
      "97/97 [==============================] - 0s 350us/sample - loss: 0.0238 - val_loss: 0.0143\n",
      "Epoch 64/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0219 - val_loss: 0.0134\n",
      "Epoch 65/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0200 - val_loss: 0.0127\n",
      "Epoch 66/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0190 - val_loss: 0.0124\n",
      "Epoch 67/200\n",
      "97/97 [==============================] - 0s 514us/sample - loss: 0.0242 - val_loss: 0.0123\n",
      "Epoch 68/200\n",
      "97/97 [==============================] - 0s 380us/sample - loss: 0.0219 - val_loss: 0.0123\n",
      "Epoch 69/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0210 - val_loss: 0.0122\n",
      "Epoch 70/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0218 - val_loss: 0.0122\n",
      "Epoch 71/200\n",
      "97/97 [==============================] - 0s 380us/sample - loss: 0.0203 - val_loss: 0.0122\n",
      "Epoch 72/200\n",
      "97/97 [==============================] - 0s 350us/sample - loss: 0.0231 - val_loss: 0.0122\n",
      "Epoch 73/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0204 - val_loss: 0.0122\n",
      "Epoch 74/200\n",
      "97/97 [==============================] - 0s 370us/sample - loss: 0.0200 - val_loss: 0.0122\n",
      "Epoch 75/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0211 - val_loss: 0.0128\n",
      "Epoch 76/200\n",
      "97/97 [==============================] - 0s 360us/sample - loss: 0.0177 - val_loss: 0.0136\n",
      "Epoch 77/200\n",
      "97/97 [==============================] - 0s 370us/sample - loss: 0.0201 - val_loss: 0.0136\n",
      "Epoch 78/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0203 - val_loss: 0.0131\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 0s 350us/sample - loss: 0.0215 - val_loss: 0.0125\n",
      "Epoch 80/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0209 - val_loss: 0.0120\n",
      "Epoch 81/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0220 - val_loss: 0.0119\n",
      "Epoch 82/200\n",
      "97/97 [==============================] - 0s 360us/sample - loss: 0.0207 - val_loss: 0.0119\n",
      "Epoch 83/200\n",
      "97/97 [==============================] - 0s 350us/sample - loss: 0.0229 - val_loss: 0.0119\n",
      "Epoch 84/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0235 - val_loss: 0.0118\n",
      "Epoch 85/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0210 - val_loss: 0.0118\n",
      "Epoch 86/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0225 - val_loss: 0.0117\n",
      "Epoch 87/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0201 - val_loss: 0.0117\n",
      "Epoch 88/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0198 - val_loss: 0.0117\n",
      "Epoch 89/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0210 - val_loss: 0.0116\n",
      "Epoch 90/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0205 - val_loss: 0.0117\n",
      "Epoch 91/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0192 - val_loss: 0.0118\n",
      "Epoch 92/200\n",
      "97/97 [==============================] - 0s 370us/sample - loss: 0.0205 - val_loss: 0.0120\n",
      "Epoch 93/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0219 - val_loss: 0.0124\n",
      "Epoch 94/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0188 - val_loss: 0.0131\n",
      "Epoch 95/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0214 - val_loss: 0.0147\n",
      "Epoch 96/200\n",
      "97/97 [==============================] - 0s 288us/sample - loss: 0.0176 - val_loss: 0.0134\n",
      "Epoch 97/200\n",
      "97/97 [==============================] - 0s 360us/sample - loss: 0.0198 - val_loss: 0.0120\n",
      "Epoch 98/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0233 - val_loss: 0.0114\n",
      "Epoch 99/200\n",
      "97/97 [==============================] - 0s 350us/sample - loss: 0.0245 - val_loss: 0.0115\n",
      "Epoch 100/200\n",
      "97/97 [==============================] - 0s 350us/sample - loss: 0.0195 - val_loss: 0.0116\n",
      "Epoch 101/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0201 - val_loss: 0.0114\n",
      "Epoch 102/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0228 - val_loss: 0.0113\n",
      "Epoch 103/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0204 - val_loss: 0.0114\n",
      "Epoch 104/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0203 - val_loss: 0.0121\n",
      "Epoch 105/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0210 - val_loss: 0.0127\n",
      "Epoch 106/200\n",
      "97/97 [==============================] - 0s 308us/sample - loss: 0.0216 - val_loss: 0.0129\n",
      "Epoch 107/200\n",
      "97/97 [==============================] - 0s 360us/sample - loss: 0.0199 - val_loss: 0.0125\n",
      "Epoch 108/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0203 - val_loss: 0.0120\n",
      "Epoch 109/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0194 - val_loss: 0.0115\n",
      "Epoch 110/200\n",
      "97/97 [==============================] - 0s 360us/sample - loss: 0.0187 - val_loss: 0.0113\n",
      "Epoch 111/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0175 - val_loss: 0.0112\n",
      "Epoch 112/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0191 - val_loss: 0.0115\n",
      "Epoch 113/200\n",
      "97/97 [==============================] - 0s 380us/sample - loss: 0.0183 - val_loss: 0.0124\n",
      "Epoch 114/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0192 - val_loss: 0.0136\n",
      "Epoch 115/200\n",
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0192 - val_loss: 0.0136\n",
      "Epoch 116/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0211 - val_loss: 0.0129\n",
      "Epoch 117/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0197 - val_loss: 0.0122\n",
      "Epoch 118/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0201 - val_loss: 0.0118\n",
      "Epoch 119/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0200 - val_loss: 0.0114\n",
      "Epoch 120/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0207 - val_loss: 0.0110\n",
      "Epoch 121/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0165 - val_loss: 0.0109\n",
      "Epoch 122/200\n",
      "97/97 [==============================] - 0s 298us/sample - loss: 0.0192 - val_loss: 0.0109\n",
      "Epoch 123/200\n",
      "97/97 [==============================] - 0s 319us/sample - loss: 0.0182 - val_loss: 0.0109\n",
      "Epoch 124/200\n",
      "97/97 [==============================] - 0s 298us/sample - loss: 0.0195 - val_loss: 0.0109\n",
      "Epoch 125/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0190 - val_loss: 0.0108\n",
      "Epoch 126/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0196 - val_loss: 0.0108\n",
      "Epoch 127/200\n",
      "97/97 [==============================] - 0s 411us/sample - loss: 0.0212 - val_loss: 0.0109\n",
      "Epoch 128/200\n",
      "97/97 [==============================] - 0s 308us/sample - loss: 0.0165 - val_loss: 0.0111\n",
      "Epoch 129/200\n",
      "97/97 [==============================] - 0s 308us/sample - loss: 0.0153 - val_loss: 0.0116\n",
      "Epoch 130/200\n",
      "97/97 [==============================] - 0s 329us/sample - loss: 0.0168 - val_loss: 0.0117\n",
      "Epoch 131/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0171 - val_loss: 0.0111\n",
      "Epoch 132/200\n",
      "97/97 [==============================] - 0s 339us/sample - loss: 0.0169 - val_loss: 0.0108\n",
      "Epoch 133/200\n",
      "97/97 [==============================] - 0s 370us/sample - loss: 0.0186 - val_loss: 0.0107\n",
      "Epoch 134/200\n",
      "97/97 [==============================] - 0s 350us/sample - loss: 0.0200 - val_loss: 0.0107\n",
      "Epoch 135/200\n",
      "97/97 [==============================] - 0s 380us/sample - loss: 0.0188 - val_loss: 0.0106\n",
      "Epoch 136/200\n",
      "97/97 [==============================] - 0s 360us/sample - loss: 0.0157 - val_loss: 0.0106\n",
      "Epoch 137/200\n",
      "97/97 [==============================] - 0s 411us/sample - loss: 0.0182 - val_loss: 0.0106\n",
      "Epoch 138/200\n",
      "97/97 [==============================] - 0s 380us/sample - loss: 0.0183 - val_loss: 0.0106\n",
      "Epoch 139/200\n",
      "97/97 [==============================] - 0s 411us/sample - loss: 0.0164 - val_loss: 0.0106\n",
      "Epoch 140/200\n",
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0171 - val_loss: 0.0106\n",
      "Epoch 141/200\n",
      "97/97 [==============================] - 0s 401us/sample - loss: 0.0190 - val_loss: 0.0106\n",
      "Epoch 142/200\n",
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0177 - val_loss: 0.0106\n",
      "Epoch 143/200\n",
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0177 - val_loss: 0.0106\n",
      "Epoch 144/200\n",
      "97/97 [==============================] - 0s 432us/sample - loss: 0.0162 - val_loss: 0.0107\n",
      "Epoch 145/200\n",
      "97/97 [==============================] - 0s 432us/sample - loss: 0.0175 - val_loss: 0.0108\n",
      "Epoch 146/200\n",
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0201 - val_loss: 0.0110\n",
      "Epoch 147/200\n",
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0200 - val_loss: 0.0122\n",
      "Epoch 148/200\n",
      "97/97 [==============================] - 0s 411us/sample - loss: 0.0179 - val_loss: 0.0136\n",
      "Epoch 149/200\n",
      "97/97 [==============================] - 0s 432us/sample - loss: 0.0177 - val_loss: 0.0136\n",
      "Epoch 150/200\n",
      "97/97 [==============================] - 0s 411us/sample - loss: 0.0207 - val_loss: 0.0128\n",
      "Epoch 151/200\n",
      "97/97 [==============================] - 0s 463us/sample - loss: 0.0168 - val_loss: 0.0117\n",
      "Epoch 152/200\n",
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0157 - val_loss: 0.0108\n",
      "Epoch 153/200\n",
      "97/97 [==============================] - 0s 432us/sample - loss: 0.0191 - val_loss: 0.0104\n",
      "Epoch 154/200\n",
      "97/97 [==============================] - 0s 452us/sample - loss: 0.0186 - val_loss: 0.0104\n",
      "Epoch 155/200\n",
      "97/97 [==============================] - 0s 442us/sample - loss: 0.0168 - val_loss: 0.0105\n",
      "Epoch 156/200\n",
      "97/97 [==============================] - 0s 483us/sample - loss: 0.0192 - val_loss: 0.0105\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0167 - val_loss: 0.0105\n",
      "Epoch 158/200\n",
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0208 - val_loss: 0.0103\n",
      "Epoch 159/200\n",
      "97/97 [==============================] - 0s 452us/sample - loss: 0.0196 - val_loss: 0.0103\n",
      "Epoch 160/200\n",
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0177 - val_loss: 0.0103\n",
      "Epoch 161/200\n",
      "97/97 [==============================] - 0s 463us/sample - loss: 0.0171 - val_loss: 0.0104\n",
      "Epoch 162/200\n",
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0166 - val_loss: 0.0104\n",
      "Epoch 163/200\n",
      "97/97 [==============================] - 0s 442us/sample - loss: 0.0197 - val_loss: 0.0105\n",
      "Epoch 164/200\n",
      "97/97 [==============================] - 0s 422us/sample - loss: 0.0178 - val_loss: 0.0104\n",
      "Epoch 165/200\n",
      "97/97 [==============================] - 0s 442us/sample - loss: 0.0179 - val_loss: 0.0103\n",
      "Epoch 166/200\n",
      "97/97 [==============================] - 0s 442us/sample - loss: 0.0165 - val_loss: 0.0103\n",
      "Epoch 167/200\n",
      "97/97 [==============================] - 0s 442us/sample - loss: 0.0194 - val_loss: 0.0102\n",
      "Epoch 168/200\n",
      "97/97 [==============================] - 0s 555us/sample - loss: 0.0189 - val_loss: 0.0102\n",
      "Epoch 169/200\n",
      "97/97 [==============================] - 0s 483us/sample - loss: 0.0179 - val_loss: 0.0102\n",
      "Epoch 170/200\n",
      "97/97 [==============================] - 0s 483us/sample - loss: 0.0184 - val_loss: 0.0102\n",
      "Epoch 171/200\n",
      "97/97 [==============================] - 0s 473us/sample - loss: 0.0179 - val_loss: 0.0102\n",
      "Epoch 172/200\n",
      "97/97 [==============================] - 0s 473us/sample - loss: 0.0150 - val_loss: 0.0102\n",
      "Epoch 173/200\n",
      "97/97 [==============================] - 0s 504us/sample - loss: 0.0160 - val_loss: 0.0102\n",
      "Epoch 174/200\n",
      "97/97 [==============================] - 0s 504us/sample - loss: 0.0168 - val_loss: 0.0102\n",
      "Epoch 175/200\n",
      "97/97 [==============================] - 0s 473us/sample - loss: 0.0168 - val_loss: 0.0103\n",
      "Epoch 176/200\n",
      "97/97 [==============================] - 0s 473us/sample - loss: 0.0177 - val_loss: 0.0103\n",
      "Epoch 177/200\n",
      "97/97 [==============================] - 0s 442us/sample - loss: 0.0170 - val_loss: 0.0104\n",
      "Epoch 178/200\n",
      "97/97 [==============================] - 0s 442us/sample - loss: 0.0167 - val_loss: 0.0104\n",
      "Epoch 179/200\n",
      "97/97 [==============================] - 0s 463us/sample - loss: 0.0158 - val_loss: 0.0104\n",
      "Epoch 180/200\n",
      "97/97 [==============================] - 0s 442us/sample - loss: 0.0152 - val_loss: 0.0104\n",
      "Epoch 181/200\n",
      "97/97 [==============================] - 0s 442us/sample - loss: 0.0190 - val_loss: 0.0103\n",
      "Epoch 182/200\n",
      "97/97 [==============================] - 0s 442us/sample - loss: 0.0199 - val_loss: 0.0102\n",
      "Epoch 183/200\n",
      "97/97 [==============================] - 0s 452us/sample - loss: 0.0163 - val_loss: 0.0101\n",
      "Epoch 184/200\n",
      "97/97 [==============================] - 0s 483us/sample - loss: 0.0160 - val_loss: 0.0101\n",
      "Epoch 185/200\n",
      "97/97 [==============================] - 0s 463us/sample - loss: 0.0200 - val_loss: 0.0101\n",
      "Epoch 186/200\n",
      "97/97 [==============================] - 0s 483us/sample - loss: 0.0179 - val_loss: 0.0100\n",
      "Epoch 187/200\n",
      "97/97 [==============================] - 0s 545us/sample - loss: 0.0187 - val_loss: 0.0100\n",
      "Epoch 188/200\n",
      "97/97 [==============================] - 0s 442us/sample - loss: 0.0190 - val_loss: 0.0099\n",
      "Epoch 189/200\n",
      "97/97 [==============================] - 0s 442us/sample - loss: 0.0167 - val_loss: 0.0100\n",
      "Epoch 190/200\n",
      "97/97 [==============================] - 0s 514us/sample - loss: 0.0157 - val_loss: 0.0100\n",
      "Epoch 191/200\n",
      "97/97 [==============================] - 0s 504us/sample - loss: 0.0164 - val_loss: 0.0100\n",
      "Epoch 192/200\n",
      "97/97 [==============================] - 0s 504us/sample - loss: 0.0161 - val_loss: 0.0099\n",
      "Epoch 193/200\n",
      "97/97 [==============================] - 0s 514us/sample - loss: 0.0157 - val_loss: 0.0099\n",
      "Epoch 194/200\n",
      "97/97 [==============================] - 0s 555us/sample - loss: 0.0163 - val_loss: 0.0100\n",
      "Epoch 195/200\n",
      "97/97 [==============================] - 0s 514us/sample - loss: 0.0151 - val_loss: 0.0101\n",
      "Epoch 196/200\n",
      "97/97 [==============================] - 0s 514us/sample - loss: 0.0162 - val_loss: 0.0107\n",
      "Epoch 197/200\n",
      "97/97 [==============================] - 0s 545us/sample - loss: 0.0187 - val_loss: 0.0112\n",
      "Epoch 198/200\n",
      "97/97 [==============================] - 0s 504us/sample - loss: 0.0154 - val_loss: 0.0117\n",
      "Epoch 199/200\n",
      "97/97 [==============================] - 0s 473us/sample - loss: 0.0181 - val_loss: 0.0122\n",
      "Epoch 200/200\n",
      "97/97 [==============================] - 0s 463us/sample - loss: 0.0182 - val_loss: 0.0119\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, epochs=200, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.ylim(0.0, 0.15)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  0.015112865683753737\n",
      "Test Score:  0.009980794369570306\n"
     ]
    }
   ],
   "source": [
    "trainScore = model.evaluate(x_train, y_train, verbose=0)\n",
    "model.reset_states()\n",
    "print('Train Score: ', trainScore)\n",
    "\n",
    "testScore = model.evaluate(x_test, y_test, verbose=0)\n",
    "model.reset_states()\n",
    "print('Test Score: ', testScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 예측 (1 주)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.102218114"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictValue = model.predict(currQty.reshape(1,look_back,1))\n",
    "predictValue[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 예측 (연속 주)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtyIn = list(test.flatten()[-10:])\n",
    "\n",
    "pred_count = 30 # 최대 예측 개수 정의\n",
    "\n",
    "pred_out = []\n",
    "\n",
    "for i in range(pred_count):\n",
    "    sample_in = np.array(qtyIn)\n",
    "    currQty = sample_in.reshape(-1,1)\n",
    "    predValue = model.predict(currQty.reshape(1,look_back,1)).flatten()[0]\n",
    "    pred_out.append(predValue)\n",
    "    qtyIn.append(predValue)\n",
    "    qtyIn.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtyIn = list(train.flatten()[-10:])\n",
    "\n",
    "pred_count = 37 # 최대 예측 개수 정의\n",
    "\n",
    "pred_out = []\n",
    "\n",
    "for i in range(pred_count):\n",
    "    sample_in = np.array(qtyIn)\n",
    "    currQty = sample_in.reshape(-1,1)\n",
    "    predValue = model.predict(currQty.reshape(1,look_back,1)).flatten()[0]\n",
    "    pred_out.append(predValue)\n",
    "    qtyIn.append(predValue)\n",
    "    qtyIn.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(np.arange(37),pred_out,'r',label=\"prediction\")\n",
    "plt.plot(np.arange(37),y_test,label=\"test function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성 (상태유지 lstm 신경망)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "# batchsize, time_step, shape\n",
    "model.add(LSTM(32, batch_input_shape=(1, look_back, 1), stateful=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 2s 25ms/sample - loss: 0.0328 - val_loss: 0.0147\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0299 - val_loss: 0.0139\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0299 - val_loss: 0.0132\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0251 - val_loss: 0.0126\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0215 - val_loss: 0.0123\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0219 - val_loss: 0.0119\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0243 - val_loss: 0.0128\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0220 - val_loss: 0.0117\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0196 - val_loss: 0.0116\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0200 - val_loss: 0.0114\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0201 - val_loss: 0.0113\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0200 - val_loss: 0.0111\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0207 - val_loss: 0.0110\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0200 - val_loss: 0.0105\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0212 - val_loss: 0.0104\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0187 - val_loss: 0.0101\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0194 - val_loss: 0.0103\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0209 - val_loss: 0.0100\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0155 - val_loss: 0.0100\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0174 - val_loss: 0.0096\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0169 - val_loss: 0.0095\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0129 - val_loss: 0.0097\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0152 - val_loss: 0.0095\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0142 - val_loss: 0.0093\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0160 - val_loss: 0.0096\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0136 - val_loss: 0.0091\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0148 - val_loss: 0.0090\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0163 - val_loss: 0.0093\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0146 - val_loss: 0.0089\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0152 - val_loss: 0.0096\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0156 - val_loss: 0.0093\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0153 - val_loss: 0.0090\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0119 - val_loss: 0.0090\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 7ms/sample - loss: 0.0128 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0129 - val_loss: 0.0090\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0110 - val_loss: 0.0088\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0117 - val_loss: 0.0088\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0112 - val_loss: 0.0088\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0121 - val_loss: 0.0091\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0134 - val_loss: 0.0089\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0115 - val_loss: 0.0090\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0128 - val_loss: 0.0085\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0140 - val_loss: 0.0100\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0110 - val_loss: 0.0098\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0113 - val_loss: 0.0091\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0110 - val_loss: 0.0082\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0107 - val_loss: 0.0084\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0140 - val_loss: 0.0093\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0129 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0101 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0112 - val_loss: 0.0089\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0118 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0115 - val_loss: 0.0083\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0098 - val_loss: 0.0089\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0126 - val_loss: 0.0093\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0103 - val_loss: 0.0096\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0094 - val_loss: 0.0093\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0095 - val_loss: 0.0088\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0087 - val_loss: 0.0088\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0113 - val_loss: 0.0094\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0114 - val_loss: 0.0099\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0115 - val_loss: 0.0084\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0102 - val_loss: 0.0101\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0102 - val_loss: 0.0080\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0101 - val_loss: 0.0088\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0118 - val_loss: 0.0086\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0086 - val_loss: 0.0090\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0109 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0089 - val_loss: 0.0088\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0089 - val_loss: 0.0094\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0072 - val_loss: 0.0089\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0096 - val_loss: 0.0092\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0087 - val_loss: 0.0093\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0126 - val_loss: 0.0083\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0111 - val_loss: 0.0083\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0093 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0106 - val_loss: 0.0082\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0102 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 7ms/sample - loss: 0.0099 - val_loss: 0.0092\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0106 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0085 - val_loss: 0.0086\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0114 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0090 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0106 - val_loss: 0.0086\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0100 - val_loss: 0.0093\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0105 - val_loss: 0.0085\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0093 - val_loss: 0.0082\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0092 - val_loss: 0.0077\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0099 - val_loss: 0.0075\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0090 - val_loss: 0.0089\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0093 - val_loss: 0.0088\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0131 - val_loss: 0.0082\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0108 - val_loss: 0.0082\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0087 - val_loss: 0.0094\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0100 - val_loss: 0.0094\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0073 - val_loss: 0.0083\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0110 - val_loss: 0.0085\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - ETA: 0s - loss: 0.008 - 1s 5ms/sample - loss: 0.0086 - val_loss: 0.0084\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0096 - val_loss: 0.0084\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0090 - val_loss: 0.0082\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0100 - val_loss: 0.0093\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0107 - val_loss: 0.0093\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0091 - val_loss: 0.0083\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0087 - val_loss: 0.0092\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0073 - val_loss: 0.0082\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0073 - val_loss: 0.0088\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0075 - val_loss: 0.0084\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0079 - val_loss: 0.0088\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0087 - val_loss: 0.0086\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0097 - val_loss: 0.0081\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0113 - val_loss: 0.0086\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0102 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0114 - val_loss: 0.0082\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0089 - val_loss: 0.0078\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0066 - val_loss: 0.0085\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0081 - val_loss: 0.0088\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0087 - val_loss: 0.0086\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0082 - val_loss: 0.0092\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0108 - val_loss: 0.0080\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0091 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0082 - val_loss: 0.0091\n",
      "Train on 97 samples, validate on 37 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0088 - val_loss: 0.0076\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0090 - val_loss: 0.0081\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0095 - val_loss: 0.0109\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 8ms/sample - loss: 0.0081 - val_loss: 0.0084\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0098 - val_loss: 0.0090\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0084 - val_loss: 0.0088\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 7ms/sample - loss: 0.0075 - val_loss: 0.0089\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0091 - val_loss: 0.0106\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0077 - val_loss: 0.0078\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0081 - val_loss: 0.0078\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0095 - val_loss: 0.0079\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0087 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0084 - val_loss: 0.0089\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0090 - val_loss: 0.0079\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0087 - val_loss: 0.0094\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0065 - val_loss: 0.0076\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0075 - val_loss: 0.0096\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 7ms/sample - loss: 0.0104 - val_loss: 0.0081\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0096 - val_loss: 0.0085\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0095 - val_loss: 0.0081\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0070 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0081 - val_loss: 0.0086\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0072 - val_loss: 0.0097\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0093 - val_loss: 0.0080\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0104 - val_loss: 0.0084\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0073 - val_loss: 0.0084\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 7ms/sample - loss: 0.0074 - val_loss: 0.0082\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0103 - val_loss: 0.0082\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 7ms/sample - loss: 0.0082 - val_loss: 0.0085\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0076 - val_loss: 0.0085\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0073 - val_loss: 0.0084\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0094 - val_loss: 0.0088\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0070 - val_loss: 0.0094\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0088 - val_loss: 0.0097\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0078 - val_loss: 0.0077\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0105 - val_loss: 0.0095\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0069 - val_loss: 0.0095\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0151 - val_loss: 0.0078\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0081 - val_loss: 0.0091\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0091 - val_loss: 0.0103\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0080 - val_loss: 0.0090\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0082 - val_loss: 0.0106\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0066 - val_loss: 0.0101\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0082 - val_loss: 0.0098\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 7ms/sample - loss: 0.0064 - val_loss: 0.0102\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0097 - val_loss: 0.0104\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0079 - val_loss: 0.0078\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0061 - val_loss: 0.0074\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0064 - val_loss: 0.0095\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0080 - val_loss: 0.0094\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0080 - val_loss: 0.0092\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0080 - val_loss: 0.0087\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0072 - val_loss: 0.0089\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0117 - val_loss: 0.0078\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0071 - val_loss: 0.0096\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0136 - val_loss: 0.0089\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0078 - val_loss: 0.0108\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0060 - val_loss: 0.0098\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0072 - val_loss: 0.0086\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0085 - val_loss: 0.0098\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0072 - val_loss: 0.0091\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0080 - val_loss: 0.0084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0066 - val_loss: 0.0077\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0066 - val_loss: 0.0090\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0056 - val_loss: 0.0077\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0077 - val_loss: 0.0091\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0121 - val_loss: 0.0069\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0066 - val_loss: 0.0105\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 8ms/sample - loss: 0.0081 - val_loss: 0.0105\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 5ms/sample - loss: 0.0076 - val_loss: 0.0112\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0067 - val_loss: 0.0086\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0060 - val_loss: 0.0107\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 1s 6ms/sample - loss: 0.0074 - val_loss: 0.0116\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0068 - val_loss: 0.0083\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0073 - val_loss: 0.0109\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0073 - val_loss: 0.0111\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0069 - val_loss: 0.0110\n",
      "Train on 97 samples, validate on 37 samples\n",
      "97/97 [==============================] - 0s 5ms/sample - loss: 0.0069 - val_loss: 0.0103\n"
     ]
    }
   ],
   "source": [
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "    model.fit(x_train, y_train, epochs=1, batch_size=1, shuffle=False, validation_data=(x_test, y_test))\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  0.005205260266697456\n",
      "Test Score:  0.01106746381309357\n"
     ]
    }
   ],
   "source": [
    "trainScore = model.evaluate(x_train, y_train, batch_size=1, verbose=0)\n",
    "model.reset_states()\n",
    "print('Train Score: ', trainScore)\n",
    "\n",
    "testScore = model.evaluate(x_test, y_test, batch_size=1, verbose=0)\n",
    "model.reset_states()\n",
    "print('Test Score: ', testScore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1주 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1344238"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictValue = model.predict(currQty.reshape(1,look_back,1))\n",
    "predictValue[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtyIn = list(test.flatten()[-10:])\n",
    "\n",
    "pred_count = 30 # 최대 예측 개수 정의\n",
    "\n",
    "pred_out = []\n",
    "\n",
    "for i in range(pred_count):\n",
    "    sample_in = np.array(qtyIn)\n",
    "    currQty = sample_in.reshape(-1,1)\n",
    "    predValue = model.predict(currQty.reshape(1,look_back,1)).flatten()[0]\n",
    "    pred_out.append(predValue)\n",
    "    qtyIn.append(predValue)\n",
    "    qtyIn.pop(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
